{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "\n",
    "#prep\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler, QuantileTransformer\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#validation libraries\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load datasets\n",
    "with open('dat/dfmain.pkl', 'rb') as pickle_file:\n",
    "    dfmain = pickle.load(pickle_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariusmiron/anaconda/envs/bias/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/mariusmiron/anaconda/envs/bias/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/mariusmiron/anaconda/envs/bias/lib/python3.6/site-packages/pandas/core/generic.py:4619: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1. , 1. , 0.5, ..., 1. , 0.5, 0.5],\n",
       "       [1. , 1. , 0.5, ..., 0. , 0. , 0.5],\n",
       "       [1. , 1. , 1. , ..., 0.5, 0.5, 0. ],\n",
       "       ...,\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 1. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.5, ..., 0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=dfmain[['recid','V65_@1_violencia_previa','V66_@2_historia_delictes_no_violents','V67_@3_inici_precoç_violencia',\\\n",
    "           'V68_@4_fracas_intervencions_anteriors','V69_@5_intents_autolesio_suicidi_anteriors','V70_@6_exposicio_violencia_llar',\\\n",
    "          'V71_@7_historia_maltracte_infantil','V72_@8_delinquencia_pares','V73_@9_separacio_precoç_pares',\\\n",
    "          'V74_@10_baix_rendiment_escola','V75_@11_delinquencia_grup_iguals','V76_@12_rebuig_grup_iguals',\\\n",
    "          'V77_@13_estrés_incapacitat_enfrontar_dificultats','V78_@14_escassa_habilitat_pares_educar','V79_@15_manca_suport_personal_social',\\\n",
    "          'V80_@16_entorn_marginal','V81_@17_actitud_negatives','V82_@18_assumpcio_riscos_impulsivitat',\\\n",
    "          'V83_@19_problemes_consum_toxics','V84_@20_problemes_maneig_enuig','V85_@21_baix_nivell_empatia_remordiment',\\\n",
    "          'V86_@22_problemes_concentracio_hiperactivitat','V87_@23_baixa_colaboracio_intervencions','V88_@24_baix_compromis_escolar_laboral']]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.replace(['Baix','Moderat','Alt'],[0,0.5,1],inplace=True)\n",
    "df['recid'].replace(['Sí','No'],[1,0],inplace=True)\n",
    "df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, axis=1).reset_index(drop=True)\n",
    "feature_cols = [col for col in df.columns if 'recid' not in col]\n",
    "y = df['recid']\n",
    "X = df[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(684, 24) (172, 24) (684,) (172,)\n"
     ]
    }
   ],
   "source": [
    "X_cls_train, X_cls_valid, y_cls_train, y_cls_valid = train_test_split(X,y, test_size=0.2)\n",
    "print(X_cls_train.shape, X_cls_valid.shape, y_cls_train.shape, y_cls_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using clasification, or logistic regression, we can always get the predictions  .predict(usually 0, 1, 2 or discrete values). But there's a second function all .predict_logproba and .predict_proba with will give a [0,1] probability for every row. This is often used to rank predictions for classification scores (listed below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why log prob vs. prob? Gradient methods generally work better optimizing logp(x)log⁡p(x) than p(x)p(x) because the gradient of logp(x)log⁡p(x) is generally more well-scaled. That is, it has a size that consistently and helpfully reflects the objective function's geometry, making it easier to select an appropriate step size and get to the optimum in fewer steps.\n",
    "\n",
    "- .predict - gives 1's and 0's\n",
    "- .predict_logproba - gives array of log probabilities, obs vs. classes\n",
    "- .predict_proba - gives array of probabilities, obs vs. classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgm = LogisticRegression()\n",
    "lgm.fit(X_cls_train,y_cls_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.563953488372093"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgm.score(X_cls_valid,y_cls_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6812865497076024"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgm.score(X_cls_train,y_cls_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_prob_0</th>\n",
       "      <th>log_prob_1</th>\n",
       "      <th>predict</th>\n",
       "      <th>prob_0</th>\n",
       "      <th>prob_1</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.376058</td>\n",
       "      <td>-1.160154</td>\n",
       "      <td>0</td>\n",
       "      <td>0.686562</td>\n",
       "      <td>0.313438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.191628</td>\n",
       "      <td>-1.746486</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825614</td>\n",
       "      <td>0.174386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.440081</td>\n",
       "      <td>-1.032780</td>\n",
       "      <td>0</td>\n",
       "      <td>0.643984</td>\n",
       "      <td>0.356016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.651993</td>\n",
       "      <td>-0.736068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.521006</td>\n",
       "      <td>0.478994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.704188</td>\n",
       "      <td>-0.682227</td>\n",
       "      <td>1</td>\n",
       "      <td>0.494510</td>\n",
       "      <td>0.505490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.285326</td>\n",
       "      <td>-1.393396</td>\n",
       "      <td>0</td>\n",
       "      <td>0.751769</td>\n",
       "      <td>0.248231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.230525</td>\n",
       "      <td>-1.580445</td>\n",
       "      <td>0</td>\n",
       "      <td>0.794117</td>\n",
       "      <td>0.205883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.299787</td>\n",
       "      <td>-1.350836</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740976</td>\n",
       "      <td>0.259024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.362826</td>\n",
       "      <td>-1.189766</td>\n",
       "      <td>0</td>\n",
       "      <td>0.695707</td>\n",
       "      <td>0.304293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.339598</td>\n",
       "      <td>-1.244991</td>\n",
       "      <td>0</td>\n",
       "      <td>0.712056</td>\n",
       "      <td>0.287944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.367278</td>\n",
       "      <td>-1.179660</td>\n",
       "      <td>0</td>\n",
       "      <td>0.692617</td>\n",
       "      <td>0.307383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.536895</td>\n",
       "      <td>-0.878419</td>\n",
       "      <td>0</td>\n",
       "      <td>0.584561</td>\n",
       "      <td>0.415439</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.468362</td>\n",
       "      <td>-0.983571</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626027</td>\n",
       "      <td>0.373973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.770891</td>\n",
       "      <td>-0.621014</td>\n",
       "      <td>1</td>\n",
       "      <td>0.462601</td>\n",
       "      <td>0.537399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.849338</td>\n",
       "      <td>-0.558088</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427698</td>\n",
       "      <td>0.572302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.598763</td>\n",
       "      <td>-0.797377</td>\n",
       "      <td>0</td>\n",
       "      <td>0.549491</td>\n",
       "      <td>0.450509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.744240</td>\n",
       "      <td>-0.644538</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475095</td>\n",
       "      <td>0.524905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.563616</td>\n",
       "      <td>-0.841988</td>\n",
       "      <td>0</td>\n",
       "      <td>0.569147</td>\n",
       "      <td>0.430853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.166745</td>\n",
       "      <td>-1.873504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.846415</td>\n",
       "      <td>0.153585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.507352</td>\n",
       "      <td>-0.921524</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602088</td>\n",
       "      <td>0.397912</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    log_prob_0  log_prob_1  predict    prob_0    prob_1  true\n",
       "0    -0.376058   -1.160154        0  0.686562  0.313438     0\n",
       "1    -0.191628   -1.746486        0  0.825614  0.174386     1\n",
       "2    -0.440081   -1.032780        0  0.643984  0.356016     0\n",
       "3    -0.651993   -0.736068        0  0.521006  0.478994     0\n",
       "4    -0.704188   -0.682227        1  0.494510  0.505490     0\n",
       "5    -0.285326   -1.393396        0  0.751769  0.248231     0\n",
       "6    -0.230525   -1.580445        0  0.794117  0.205883     0\n",
       "7    -0.299787   -1.350836        0  0.740976  0.259024     1\n",
       "8    -0.362826   -1.189766        0  0.695707  0.304293     0\n",
       "9    -0.339598   -1.244991        0  0.712056  0.287944     0\n",
       "10   -0.367278   -1.179660        0  0.692617  0.307383     0\n",
       "11   -0.536895   -0.878419        0  0.584561  0.415439     1\n",
       "12   -0.468362   -0.983571        0  0.626027  0.373973     1\n",
       "13   -0.770891   -0.621014        1  0.462601  0.537399     1\n",
       "14   -0.849338   -0.558088        1  0.427698  0.572302     1\n",
       "15   -0.598763   -0.797377        0  0.549491  0.450509     0\n",
       "16   -0.744240   -0.644538        1  0.475095  0.524905     1\n",
       "17   -0.563616   -0.841988        0  0.569147  0.430853     0\n",
       "18   -0.166745   -1.873504        0  0.846415  0.153585     0\n",
       "19   -0.507352   -0.921524        0  0.602088  0.397912     1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lgm_p = lgm.predict(X_cls_train)\n",
    "y_lgm_lpr = lgm.predict_log_proba(X_cls_train)\n",
    "y_lgm_pr = lgm.predict_proba(X_cls_train)\n",
    "\n",
    "y_lgm_lpr[:,0]\n",
    "y_lgm_pr[:,0]\n",
    "y_lgm_pr[:,1]\n",
    "pd.DataFrame({'true': y_cls_train.values,\n",
    "              'predict':y_lgm_p, \n",
    "              'log_prob_0':y_lgm_lpr[:,0],\n",
    "              'log_prob_1':y_lgm_lpr[:,1],\n",
    "              'prob_0': y_lgm_pr[:,0],\n",
    "              'prob_1': y_lgm_pr[:,1]\n",
    "             }).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".coef_ for linear models these are the coefficients that are assigned to your different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big impact not same as Better predictions A larger coefficient simple means it has a strong weight in calculating predictions, but that could lead to large error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64  0.34  0.46  0.12 -0.1   0.17 -0.01  0.45  0.37 -0.07  0.14 -0.07\n",
      "   0.    0.06 -0.24 -0.04 -0.25  0.13  0.    0.25 -0.03 -0.18  0.19  0.07]]\n"
     ]
    }
   ],
   "source": [
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[93 20]\n",
      " [42 17]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = lm.predict(X_valid)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "cnf_matrix = metrics.confusion_matrix(y_valid, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['No', 'Si'],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn metrics - good ones to know\n",
    "\n",
    "\n",
    "- metrics.accuracy_score(y_true, y_pred[, …]) Accuracy classification score.\n",
    "- metrics.auc(x, y[, reorder]) Compute Area Under the Curve (AUC) using the trapezoidal rule\n",
    "- metrics.classification_report(y_true, y_pred) Build a text report showing the main classification metrics\n",
    "- metrics.confusion_matrix(y_true, y_pred[, …]) Compute confusion matrix to evaluate the accuracy of a classification\n",
    "- metrics.f1_score(y_true, y_pred[, labels, …]) Compute the F1 score, also known as balanced F-score or F-measure\n",
    "- metrics.fbeta_score(y_true, y_pred, beta[, …]) Compute the F-beta score\n",
    "- metrics.hinge_loss(y_true, pred_decision[, …]) Average hinge loss (non-regularized)\n",
    "- metrics.log_loss(y_true, y_pred[, eps, …]) Log loss, aka logistic loss or cross-entropy loss.\n",
    "- metrics.precision_recall_curve(y_true, …) Compute precision-recall pairs for different probability thresholds\n",
    "- metrics.precision_recall_fscore_support(…) Compute precision, recall, F-measure and support for each class\n",
    "- metrics.precision_score(y_true, y_pred[, …]) Compute the precision\n",
    "- metrics.recall_score(y_true, y_pred[, …]) Compute the recall\n",
    "- metrics.roc_auc_score(y_true, y_score[, …]) Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "- metrics.roc_curve(y_true, y_score[, …]) Compute Receiver operating characteristic (ROC)\n",
    "- metrics.zero_one_loss(y_true, y_pred[, …]) Zero-one classification loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  11.008163141040425\n",
      "Accuracy_score:  0.6812865497076024\n",
      "confusion_matrix:  [[377 163]\n",
      " [ 55  89]]\n",
      "Classification_Report:               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.70      0.78       540\n",
      "          1       0.35      0.62      0.45       144\n",
      "\n",
      "avg / total       0.76      0.68      0.71       684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, auc, classification_report, \\\n",
    "confusion_matrix, f1_score, log_loss, precision_recall_curve, roc_auc_score, roc_curve\n",
    "\n",
    "print('Log Loss: ', log_loss(y_lgm_p, y_cls_train))\n",
    "print('Accuracy_score: ', accuracy_score(y_lgm_p, y_cls_train))\n",
    "print('confusion_matrix: ', confusion_matrix(y_lgm_p, y_cls_train))\n",
    "print('Classification_Report: ', classification_report(y_lgm_p, y_cls_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search/Randomized Search: the quest for hyperparameters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how many options are in logistic regression:\n",
    "   \n",
    "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "             verbose=0, warm_start=False)\n",
    "             \n",
    "Many of the advanced machine learning functions have a large number of model options that can be entered. these are often called hyper parameters. These address questions such as:\n",
    "\n",
    "\"how long should the model run\", or\n",
    "\"how many times should my computer re-look at the data\" or\n",
    "\"how slow should the computer work through the problem?\"\n",
    "\n",
    "To assist answering some of these questions, sklearn has GridSearch and RandomizedSearch which will try various combinations with a provided model, compare scores and return the optimal model that should be tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearch we can find the optimal parameters for Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 2, 'max_features': 0.25} 0.026895482916190438\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_features': [0.25, 0.5, 0.7],\n",
    "    'max_depth' : [ 2,5,10,20]\n",
    "}\n",
    "gs = GridSearchCV(cv=5, param_grid=params, estimator=rfr, verbose=0)\n",
    "gs.fit(X_train,y_train)\n",
    "print(gs.best_params_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RandomSearch we can find the optimal parameters for Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 0.5, 'max_depth': 2} 0.025731646676832202\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_features': [0.25, 0.5, 0.7],\n",
    "    'max_depth' : [ 2,5,10,20]\n",
    "}\n",
    "rs = RandomizedSearchCV(cv=5, param_distributions=params, estimator=rfr, verbose=0)\n",
    "rs.fit(X_train,y_train)\n",
    "print(rs.best_params_, rs.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
